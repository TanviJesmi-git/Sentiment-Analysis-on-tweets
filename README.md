# **Sentiment Analysis on tweets (using BiLSTM)**

## **ğŸ“Œ Introduction**

This project focuses on building a **Sentiment Analysis Model** that classifies tweets as **positive** or **negative** using Deep Learning and Natural Language Processing (NLP) techniques.
The model is trained on the **Sentiment140** dataset, which contains **1.6 million tweets**, making it ideal for large-scale text sentiment prediction tasks.

The project demonstrates essential components of NLP pipelines, including data cleaning, tokenization, sequence modeling using LSTMs, and evaluation using performance metrics and visualizations.

---

## **âœ¨ Features**

* âœ”ï¸ **Automated text preprocessing** (removal of URLs, mentions, hashtags, special characters)
* âœ”ï¸ **Stemming and stopword removal** using NLTK
* âœ”ï¸ **Deep Learning model** using **Bidirectional LSTM**
* âœ”ï¸ **Embedding layer** for dense word representation
* âœ”ï¸ **Train/Validation/Test split** for robust evaluation
* âœ”ï¸ **Performance visualization** (accuracy, loss plots)
* âœ”ï¸ **Confusion matrix & classification report**
* âœ”ï¸ **Custom prediction function** for classifying new tweets
* âœ”ï¸ **High accuracy** with regularization, dropout, and optimization techniques

---

## **ğŸ“‚ Dataset**

### **Sentiment140 Dataset (Kaggle)**

* **Total tweets:** 1,600,000
* **Labels:**

  * `0` â†’ Negative
  * `4` â†’ Positive (converted to `1` during preprocessing)

### **Dataset Contents**

* Original tweet text
* Polarity label
* Metadata (IDs, timestamps, query info â€” not used)

### **Preprocessing Steps**

* Remove URLs
* Remove mentions (`@username`)
* Remove hashtags (`#happy â†’ happy`)
* Remove non-alphabetic characters
* Convert to lowercase
* Tokenization
* Stopword removal
* Stemming using **PorterStemmer**

The cleaned text is stored in a new column: **stemmed_content**.

---

## **ğŸš€ Getting Started**

### **1. Clone the Repository**

```bash
git clone https://github.com/your-username/sentiment-analysis-lstm.git
cd sentiment-analysis-lstm
```

### **2. Install Dependencies**

```bash
pip install -r requirements.txt
```

Dependencies include:

* TensorFlow / Keras
* NLTK
* scikit-learn
* Matplotlib, Seaborn
* pandas, numpy

### **3. Download Dataset**

Download the Sentiment140 dataset from Kaggle:

```
kaggle datasets download -d kazanova/sentiment140
```

Extract:

```bash
unzip sentiment140.zip
```

### **4. Run the Training Script**

```bash
python train.py
```

### **5. Predict Sentiment on Custom Input**

Use the built-in prediction function:

```python
predict_sentiment(["I love this product!", "This is horrible."])
```

---

## **ğŸ§  Model Specification**

### **Model Architecture**

| Layer                      | Description                                         |
| -------------------------- | --------------------------------------------------- |
| **Embedding (128-dim)**    | Converts tokens to dense vector representations     |
| **SpatialDropout1D (0.3)** | Regularization for embedding layer                  |
| **Bi-LSTM (128 units)**    | Captures bidirectional sequence context             |
| **Bi-LSTM (64 units)**     | Additional layer for deeper learning                |
| **Dense (32, ReLU)**       | Fully connected hidden layer with L2 regularization |
| **Dropout (0.5)**          | Prevents overfitting                                |
| **Dense (1, Sigmoid)**     | Output layer for binary classification              |

### **Hyperparameters**

* **Tokenizer vocabulary size:** 50,000
* **Sequence length:** 50 tokens
* **Optimizer:** Adam (lr = 0.0005)
* **Loss:** Binary Crossentropy
* **Batch size:** 512
* **Epochs:** 12
* **Callbacks:**

  * EarlyStopping
  * ReduceLROnPlateau

---

## **ğŸ“Š Results**

### **Evaluation Metrics**

* **Accuracy:** *(Add final accuracy observed during training)*
* **Classification Report:**

  * Precision
  * Recall
  * F1-Score

### **Confusion Matrix**

Displays True Positive, True Negative, False Positive, False Negative counts.

### **Training Curves**

Two graphs:

* **Accuracy vs Epochs**
* **Loss vs Epochs**

These help visualize learning behavior and overfitting.

### **Sample Predictions**

```
Tweet: I absolutely love this new phone!
â†’ Positive ğŸ˜€ (Confidence: 0.982)

Tweet: This is the worst movie ever.
â†’ Negative ğŸ˜¡ (Confidence: 0.876)
```

---

## **ğŸ“š References**

* Sentiment140 Dataset â€” Kaggle
* TensorFlow Documentation
* NLTK Stopword Corpus
* scikit-learn
* Research papers on **LSTM** and **Word Embeddings**
